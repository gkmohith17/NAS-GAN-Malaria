{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, LeakyReLU\nfrom tensorflow.keras.layers import Input, Reshape, UpSampling2D, BatchNormalization, Activation\nfrom tensorflow.keras.layers import Concatenate, Add, GlobalAveragePooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport random\nfrom copy import deepcopy\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport time\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-25T02:33:47.612650Z","iopub.execute_input":"2025-04-25T02:33:47.613125Z","iopub.status.idle":"2025-04-25T02:34:00.674822Z","shell.execute_reply.started":"2025-04-25T02:33:47.613103Z","shell.execute_reply":"2025-04-25T02:34:00.673951Z"}},"outputs":[{"name":"stderr","text":"2025-04-25 02:33:48.983377: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745548429.178973      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745548429.234560      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Set seed for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\nrandom.seed(42)\n\n# Set paths for the dataset\ntrain_dir = '/kaggle/input/malaria-detection-dataset/Dataset/Train'\ntest_dir = '/kaggle/input/malaria-detection-dataset/Dataset/Test'\nparasite_train_dir = os.path.join(train_dir, 'Parasite')\nuninfected_train_dir = os.path.join(train_dir, 'Uninfected')\nparasite_test_dir = os.path.join(test_dir, 'Parasite')\nuninfected_test_dir = os.path.join(test_dir, 'Uninfected')\n\n# Configuration parameters\nclass Config:\n    # Image parameters\n    img_width, img_height = 64, 64\n    channels = 3\n    \n    # GAN parameters\n    latent_dim = 100\n    batch_size = 32\n    \n    # NAS parameters\n    population_size = 10\n    generations = 5\n    mutation_rate = 0.2\n    tournament_size = 3\n    \n    # Training parameters\n    nas_epochs = 3  # Short training for each candidate architecture\n    final_epochs = 10  # Longer training for the best architecture\n    standard_gan_epochs = 10  # Training for standard GAN\n    \n    # Operation pool\n    operations = [\n        'conv3x3', 'conv5x5', 'conv7x7',\n        'sep_conv3x3', 'sep_conv5x5',\n        'dil_conv3x3', 'dil_conv5x5',\n        'skip_connect', 'none'\n    ]\n    \n    # Cell structure\n    cell_nodes = 4  # Number of nodes in a cell\n    max_edges = 2   # Maximum incoming edges per node\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T02:34:00.675998Z","iopub.execute_input":"2025-04-25T02:34:00.676516Z","iopub.status.idle":"2025-04-25T02:34:00.682749Z","shell.execute_reply.started":"2025-04-25T02:34:00.676490Z","shell.execute_reply":"2025-04-25T02:34:00.682031Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Data loading and preprocessing functions\ndef create_data_generators():\n    train_datagen = ImageDataGenerator(\n        rescale=1./255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True\n    )\n    \n    test_datagen = ImageDataGenerator(rescale=1./255)\n    \n    train_generator = train_datagen.flow_from_directory(\n        train_dir,\n        target_size=(Config.img_width, Config.img_height),\n        batch_size=Config.batch_size,\n        class_mode='binary',\n        color_mode='rgb'\n    )\n    \n    test_generator = test_datagen.flow_from_directory(\n        test_dir,\n        target_size=(Config.img_width, Config.img_height),\n        batch_size=Config.batch_size,\n        class_mode='binary',\n        color_mode='rgb',\n        shuffle=False\n    )\n    \n    return train_generator, test_generator\n\ndef load_data_from_generator(generator):\n    \"\"\"Load data from a generator and scale to [-1, 1] for tanh activation.\"\"\"\n    generator.reset()\n    X_data = []\n    y_data = []\n    \n    # Get all data from generator\n    for _ in range(int(np.ceil(generator.samples / Config.batch_size))):\n        X, y = next(generator)\n        X_data.extend(X)\n        y_data.extend(y)\n        if len(X_data) >= generator.samples:\n            break\n    \n    X_data = np.array(X_data)\n    y_data = np.array(y_data)\n    \n    # Scale data from [0, 1] to [-1, 1] for tanh activation\n    X_data = X_data * 2 - 1\n    \n    return X_data, y_data\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T02:34:00.683338Z","iopub.execute_input":"2025-04-25T02:34:00.683606Z","iopub.status.idle":"2025-04-25T02:34:00.909167Z","shell.execute_reply.started":"2025-04-25T02:34:00.683583Z","shell.execute_reply":"2025-04-25T02:34:00.908387Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Define building blocks for the NAS search space\nclass NASOperations:\n    @staticmethod\n    def conv_op(x, filters, kernel_size, name):\n        return Conv2D(filters, kernel_size, padding='same', name=f\"{name}_conv{kernel_size}\")(x)\n    \n    @staticmethod\n    def sep_conv_op(x, filters, kernel_size, name):\n        return tf.keras.layers.SeparableConv2D(\n            filters, kernel_size, padding='same', name=f\"{name}_sep_conv{kernel_size}\"\n        )(x)\n    \n    @staticmethod\n    def dil_conv_op(x, filters, kernel_size, name):\n        return Conv2D(\n            filters, kernel_size, padding='same', dilation_rate=2, \n            name=f\"{name}_dil_conv{kernel_size}\"\n        )(x)\n    \n    @staticmethod\n    def skip_connect(x, filters, name):\n        if x.shape[-1] != filters:\n            return Conv2D(filters, 1, padding='same', name=f\"{name}_skip_proj\")(x)\n        return x\n    \n    @staticmethod\n    def apply_operation(x, op_name, filters, node_id):\n        name = f\"node{node_id}_{op_name}\"\n        \n        if op_name == 'conv3x3':\n            return NASOperations.conv_op(x, filters, 3, name)\n        elif op_name == 'conv5x5':\n            return NASOperations.conv_op(x, filters, 5, name)\n        elif op_name == 'conv7x7':\n            return NASOperations.conv_op(x, filters, 7, name)\n        elif op_name == 'sep_conv3x3':\n            return NASOperations.sep_conv_op(x, filters, 3, name)\n        elif op_name == 'sep_conv5x5':\n            return NASOperations.sep_conv_op(x, filters, 5, name)\n        elif op_name == 'dil_conv3x3':\n            return NASOperations.dil_conv_op(x, filters, 3, name)\n        elif op_name == 'dil_conv5x5':\n            return NASOperations.dil_conv_op(x, filters, 5, name)\n        elif op_name == 'skip_connect':\n            return NASOperations.skip_connect(x, filters, name)\n        elif op_name == 'none':\n            return None\n        else:\n            raise ValueError(f\"Unknown operation: {op_name}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T02:34:00.911116Z","iopub.execute_input":"2025-04-25T02:34:00.911439Z","iopub.status.idle":"2025-04-25T02:34:00.923847Z","shell.execute_reply.started":"2025-04-25T02:34:00.911420Z","shell.execute_reply":"2025-04-25T02:34:00.923251Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Cell genotype definition\nclass CellGenotype:\n    def __init__(self, edges=None, ops=None):\n        # Initialize with random architecture if not provided\n        if edges is None or ops is None:\n            self.edges = []\n            self.ops = []\n            \n            # For each node in the cell (excluding input nodes)\n            for node_id in range(2, 2 + Config.cell_nodes):\n                node_edges = []\n                node_ops = []\n                \n                # Randomly select incoming edges (from previous nodes)\n                num_edges = random.randint(1, min(node_id, Config.max_edges))\n                possible_inputs = list(range(0, node_id))\n                selected_inputs = random.sample(possible_inputs, num_edges)\n                \n                for input_id in selected_inputs:\n                    node_edges.append(input_id)\n                    # Randomly select an operation (excluding 'none' for initial random sampling)\n                    op = random.choice(Config.operations[:-1])\n                    node_ops.append(op)\n                \n                self.edges.append(node_edges)\n                self.ops.append(node_ops)\n        else:\n            self.edges = edges\n            self.ops = ops\n    \n    def mutate(self):\n        \"\"\"Mutate the cell architecture.\"\"\"\n        mutated = deepcopy(self)\n        \n        # Randomly choose what to mutate\n        mutation_type = random.choice(['edge', 'op'])\n        \n        if mutation_type == 'edge':\n            # Choose a random node to mutate its edges\n            node_idx = random.randint(0, len(mutated.edges) - 1)\n            \n            # If we have more than 1 edge, we might remove one\n            if len(mutated.edges[node_idx]) > 1 and random.random() < 0.5:\n                edge_to_remove = random.randint(0, len(mutated.edges[node_idx]) - 1)\n                mutated.edges[node_idx].pop(edge_to_remove)\n                mutated.ops[node_idx].pop(edge_to_remove)\n            # Otherwise, add a new edge if possible\n            elif len(mutated.edges[node_idx]) < min(2 + node_idx, Config.max_edges):\n                # Find possible input nodes that are not already connected\n                possible_inputs = [i for i in range(0, 2 + node_idx) \n                                  if i not in mutated.edges[node_idx]]\n                \n                if possible_inputs:\n                    new_input = random.choice(possible_inputs)\n                    mutated.edges[node_idx].append(new_input)\n                    # Choose a random operation\n                    new_op = random.choice(Config.operations[:-1])\n                    mutated.ops[node_idx].append(new_op)\n        \n        else:  # Mutate operation\n            # Choose a random node\n            node_idx = random.randint(0, len(mutated.ops) - 1)\n            \n            if mutated.ops[node_idx]:  # Ensure there are operations to mutate\n                # Choose a random operation in that node\n                op_idx = random.randint(0, len(mutated.ops[node_idx]) - 1)\n                \n                # Replace with a different operation\n                current_op = mutated.ops[node_idx][op_idx]\n                available_ops = [op for op in Config.operations[:-1] if op != current_op]\n                mutated.ops[node_idx][op_idx] = random.choice(available_ops)\n                \n        return mutated\n    \n    def crossover(self, other):\n        \"\"\"Perform crossover with another cell genotype.\"\"\"\n        # Ensure both parents have the same structure\n        if len(self.edges) != len(other.edges):\n            raise ValueError(\"Parents must have the same number of nodes\")\n        \n        child_edges = []\n        child_ops = []\n        \n        # For each node, randomly choose edges and ops from either parent\n        for i in range(len(self.edges)):\n            if random.random() < 0.5:\n                child_edges.append(deepcopy(self.edges[i]))\n                child_ops.append(deepcopy(self.ops[i]))\n            else:\n                child_edges.append(deepcopy(other.edges[i]))\n                child_ops.append(deepcopy(other.ops[i]))\n        \n        return CellGenotype(edges=child_edges, ops=child_ops)\n    \n    def to_string(self):\n        \"\"\"Convert genotype to string representation.\"\"\"\n        result = []\n        for node_idx, (node_edges, node_ops) in enumerate(zip(self.edges, self.ops)):\n            node_str = f\"Node {node_idx+2}: \"\n            for edge, op in zip(node_edges, node_ops):\n                node_str += f\"({edge}->{node_idx+2}, {op}) \"\n            result.append(node_str)\n        return \"\\n\".join(result)\n    \n    def __str__(self):\n        return self.to_string()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T02:34:00.924488Z","iopub.execute_input":"2025-04-25T02:34:00.924722Z","iopub.status.idle":"2025-04-25T02:34:00.939894Z","shell.execute_reply.started":"2025-04-25T02:34:00.924701Z","shell.execute_reply":"2025-04-25T02:34:00.939183Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Generator with NAS architecture\nclass NASGANGenerator:\n    def __init__(self, genotype=None):\n        self.genotype = genotype if genotype else CellGenotype()\n    \n    def build_model(self):\n        \"\"\"Build generator model based on genotype.\"\"\"\n        # Input noise\n        noise_input = Input(shape=(Config.latent_dim,))\n        \n        # Initial dense and reshape\n        x = Dense(8*8*256, activation=\"relu\")(noise_input)\n        x = Reshape((8, 8, 256))(x)\n        \n        # Apply cells at different resolutions\n        filter_sizes = [256, 128, 64, 32]\n        \n        for i, filters in enumerate(filter_sizes):\n            # Apply NAS cell\n            x = self._build_cell(x, filters, f\"cell_{i}\")\n            \n            # Upsampling (except for the last level)\n            if i < len(filter_sizes) - 1:\n                x = UpSampling2D()(x)\n        \n        # Final output layer\n        out = Conv2D(Config.channels, kernel_size=3, padding=\"same\", activation=\"tanh\")(x)\n        \n        # Create and return model\n        return Model(noise_input, out)\n    \n    def _build_cell(self, x, filters, name_prefix):\n        \"\"\"Build a single cell based on genotype.\"\"\"\n        nodes = [None] * (2 + Config.cell_nodes)\n        \n        # First two nodes are cell inputs\n        nodes[0] = x\n        nodes[1] = x  # In a more complex setup, this could be a different input\n        \n        # Process each intermediate node\n        for node_idx, (node_edges, node_ops) in enumerate(zip(self.genotype.edges, self.genotype.ops)):\n            actual_node_idx = node_idx + 2  # Offset for input nodes\n            \n            node_inputs = []\n            for edge_idx, (input_idx, op_name) in enumerate(zip(node_edges, node_ops)):\n                # Apply operation to input\n                processed = NASOperations.apply_operation(\n                    nodes[input_idx], \n                    op_name, \n                    filters, \n                    f\"{name_prefix}_n{actual_node_idx}_e{edge_idx}\"\n                )\n                \n                if processed is not None:  # Skip 'none' operations\n                    node_inputs.append(processed)\n            \n            # Combine inputs if there are any\n            if node_inputs:\n                if len(node_inputs) == 1:\n                    combined = node_inputs[0]\n                else:\n                    combined = Add()(node_inputs)\n                \n                # Apply activation and batch norm\n                combined = BatchNormalization()(combined)\n                combined = LeakyReLU(alpha=0.2)(combined)\n                \n                nodes[actual_node_idx] = combined\n            else:\n                # If no inputs, use skip connection from previous node\n                nodes[actual_node_idx] = nodes[actual_node_idx-1]\n        \n        # Use the last node as cell output\n        return nodes[-1]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T02:34:00.940476Z","iopub.execute_input":"2025-04-25T02:34:00.940713Z","iopub.status.idle":"2025-04-25T02:34:00.956831Z","shell.execute_reply.started":"2025-04-25T02:34:00.940692Z","shell.execute_reply":"2025-04-25T02:34:00.956194Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Standard GAN implementation\nclass StandardGAN:\n    def __init__(self, X_train):\n        self.X_train = X_train\n        self.latent_dim = Config.latent_dim\n        self.img_shape = (Config.img_width, Config.img_height, Config.channels)\n        \n        # Build and compile the discriminator\n        self.discriminator = self._build_discriminator()\n        self.discriminator.compile(\n            loss='binary_crossentropy',\n            optimizer=Adam(0.0002, 0.5),\n            metrics=['accuracy']\n        )\n        \n        # Build the generator\n        self.generator = self._build_generator()\n        \n        # For the combined model, we will only train the generator\n        self.discriminator.trainable = False\n        \n        # The generator takes noise as input and generates images\n        z = Input(shape=(self.latent_dim,))\n        img = self.generator(z)\n        \n        # The discriminator takes generated images as input and determines validity\n        valid = self.discriminator(img)\n        \n        # The combined model (stacked generator and discriminator)\n        self.combined = Model(z, valid)\n        self.combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n    \n    def _build_generator(self):\n        model = Sequential()\n        \n        # Foundation for 8x8 feature maps\n        model.add(Dense(8*8*256, activation=\"relu\", input_dim=self.latent_dim))\n        model.add(Reshape((8, 8, 256)))\n        \n        # Upsampling to 16x16\n        model.add(UpSampling2D())\n        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        \n        # Upsampling to 32x32\n        model.add(UpSampling2D())\n        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        \n        # Upsampling to 64x64\n        model.add(UpSampling2D())\n        model.add(Conv2D(32, kernel_size=3, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        \n        # Output layer with 3 channels (RGB)\n        model.add(Conv2D(3, kernel_size=3, padding=\"same\", activation=\"tanh\"))\n        \n        noise = Input(shape=(self.latent_dim,))\n        img = model(noise)\n        \n        return Model(noise, img)\n    \n    def _build_discriminator(self):\n        model = Sequential()\n        \n        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n        model.add(LeakyReLU(negative_slope=0.2))  # Changed from alpha to negative_slope\n        model.add(Dropout(0.25))\n        \n        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(negative_slope=0.2))  # Changed from alpha to negative_slope\n        model.add(Dropout(0.25))\n        \n        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(negative_slope=0.2))  # Changed from alpha to negative_slope\n        model.add(Dropout(0.25))\n        \n        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(negative_slope=0.2))  # Changed from alpha to negative_slope\n        model.add(Dropout(0.25))\n        \n        model.add(Flatten())\n        model.add(Dense(1, activation='sigmoid'))\n        \n        return model\n    \n    def train(self, epochs, batch_size=32, save_interval=5, save_prefix=\"standard\"):\n        # Save training history\n        d_losses = []\n        g_losses = []\n        d_accs = []\n        \n        for epoch in range(epochs):\n            # ---------------------\n            #  Train Discriminator\n            # ---------------------\n            \n            # Select a random batch of images\n            idx = np.random.randint(0, self.X_train.shape[0], batch_size // 2)\n            imgs = self.X_train[idx]\n            \n            # Generate a batch of new images\n            noise = np.random.normal(0, 1, (batch_size // 2, self.latent_dim))\n            gen_imgs = self.generator.predict(noise)\n            \n            # Train the discriminator\n            d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((batch_size // 2, 1)))\n            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((batch_size // 2, 1)))\n            \n            # Check if d_loss is a list/tuple with two elements or just a single value\n            if isinstance(d_loss_real, (list, tuple)) and len(d_loss_real) > 1:\n                d_loss = [0.5 * (d_loss_real[0] + d_loss_fake[0]), 0.5 * (d_loss_real[1] + d_loss_fake[1])]\n                d_loss_value = d_loss[0]\n                d_acc_value = d_loss[1]\n            else:\n                d_loss = 0.5 * (d_loss_real + d_loss_fake)\n                d_loss_value = d_loss\n                d_acc_value = 0.5  # Default accuracy if not provided\n            \n            # ---------------------\n            #  Train Generator\n            # ---------------------\n            \n            # Train the generator to fool the discriminator\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = self.combined.train_on_batch(noise, np.ones((batch_size, 1)))\n            \n            # Save losses and accuracy for plotting\n            d_losses.append(d_loss_value)\n            d_accs.append(d_acc_value)\n            g_losses.append(g_loss)\n            \n            # Print progress\n            g_loss_value = g_loss[0] if isinstance(g_loss, (list, tuple)) else g_loss\n            print(f\"{epoch}/{epochs} [D loss: {d_loss_value:.4f}, acc.: {100*d_acc_value:.2f}%] [G loss: {g_loss_value:.4f}]\")\n            \n            \n            # If at save interval => save generated image samples\n            if epoch % save_interval == 0:\n                self.save_imgs(epoch, save_prefix)\n        \n        # Plot training history\n        self.plot_history(d_losses, g_losses, d_accs, save_prefix)\n        \n        return d_losses, g_losses, d_accs\n        \n    def save_imgs(self, epoch, prefix, examples=10):\n        \"\"\"Save generated images.\"\"\"\n        noise = np.random.normal(0, 1, (examples, self.latent_dim))\n        generated_images = self.generator.predict(noise)\n        \n        # Rescale images from [-1, 1] to [0, 1]\n        generated_images = (generated_images + 1) / 2.0\n        \n        plt.figure(figsize=(10, 4))\n        for i in range(examples):\n            plt.subplot(2, 5, i+1)\n            plt.imshow(generated_images[i])\n            plt.axis('off')\n        plt.tight_layout()\n        plt.savefig(f'{prefix}_gan_generated_image_epoch_{epoch}.png')\n        plt.close()\n        \n        return generated_images\n    \n    def plot_history(self, d_losses, g_losses, d_accs, prefix):\n        plt.figure(figsize=(15, 5))\n        \n        # Plot discriminator loss\n        plt.subplot(1, 3, 1)\n        plt.plot(d_losses)\n        plt.title('Discriminator Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        \n        # Plot generator loss\n        plt.subplot(1, 3, 2)\n        plt.plot(g_losses)\n        plt.title('Generator Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        \n        # Plot discriminator accuracy\n        plt.subplot(1, 3, 3)\n        plt.plot(d_accs)\n        plt.title('Discriminator Accuracy')\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy')\n        \n        plt.tight_layout()\n        plt.savefig(f'{prefix}_gan_training_history.png')\n        plt.close()\n    \n    def generate_augmented_images(self, num_images=5000):\n        \"\"\"Generate augmented images.\"\"\"\n        noise = np.random.normal(0, 1, (num_images, self.latent_dim))\n        generated_images = self.generator.predict(noise)\n        \n        # Convert from [-1, 1] to [0, 1]\n        generated_images = (generated_images + 1) / 2.0\n        \n        return generated_images\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T02:34:00.957576Z","iopub.execute_input":"2025-04-25T02:34:00.957852Z","iopub.status.idle":"2025-04-25T02:34:00.979595Z","shell.execute_reply.started":"2025-04-25T02:34:00.957830Z","shell.execute_reply":"2025-04-25T02:34:00.979077Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# NAS-GAN implementation using evolutionary search\nclass NASGAN:\n    def __init__(self, X_train):\n        self.X_train = X_train\n        self.latent_dim = Config.latent_dim\n        self.population = []\n        self.fitness_history = []\n        self.best_genotype = None\n        self.best_fitness = -float('inf')\n        self.img_shape = (Config.img_width, Config.img_height, Config.channels)\n    \n    def initialize_population(self):\n        \"\"\"Initialize random population of architectures.\"\"\"\n        self.population = [CellGenotype() for _ in range(Config.population_size)]\n    \n    def _build_discriminator(self):\n        \"\"\"Build a fixed discriminator architecture.\"\"\"\n        model = Sequential()\n        \n        model.add(Conv2D(32, kernel_size=3, strides=2, \n                         input_shape=self.img_shape, \n                         padding=\"same\"))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        \n        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        \n        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        \n        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        \n        model.add(Flatten())\n        model.add(Dense(1, activation='sigmoid'))\n        \n        return model\n    \n    def evaluate_fitness(self, genotype, epochs=3):\n        \"\"\"Evaluate fitness of a single genotype.\"\"\"\n        # Build generator with this genotype\n        nas_gen = NASGANGenerator(genotype)\n        generator = nas_gen.build_model()\n        \n        # Build discriminator (fixed architecture)\n        discriminator = self._build_discriminator()\n        discriminator.compile(\n            loss='binary_crossentropy',\n            optimizer=Adam(0.0002, 0.5),\n            metrics=['accuracy']\n        )\n        \n        # Create GAN\n        discriminator.trainable = False\n        z = Input(shape=(self.latent_dim,))\n        img = generator(z)\n        valid = discriminator(img)\n        combined = Model(z, valid)\n        combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n        \n        # Train for a few epochs to assess performance\n        batch_size = Config.batch_size\n        half_batch = batch_size // 2\n        \n        # Keep track of metrics\n        d_losses = []\n        g_losses = []\n        \n        for epoch in range(epochs):\n            # Train discriminator\n            idx = np.random.randint(0, self.X_train.shape[0], half_batch)\n            real_imgs = self.X_train[idx]\n            \n            noise = np.random.normal(0, 1, (half_batch, self.latent_dim))\n            gen_imgs = generator.predict(noise)\n            \n            d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((half_batch, 1)))\n            d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            \n            # Train generator\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = combined.train_on_batch(noise, np.ones((batch_size, 1)))\n            \n            d_losses.append(d_loss[0])\n            g_losses.append(g_loss)\n        \n        # Use generator loss stability and discriminator accuracy as fitness\n        # Lower generator loss is better, but we also want the discriminator to be accurate\n        g_loss_stability = -np.std(g_losses[-5:]) if len(g_losses) >= 5 else -np.std(g_losses)\n        d_accuracy = np.mean([d[1] for d in d_losses]) if isinstance(d_losses[0], (list, tuple)) else 0.5\n        \n        # A fitness function that balances generator quality and training stability\n        fitness = g_loss_stability - abs(0.7 - d_accuracy) * 2\n        \n        # Clean up to prevent memory leaks\n        tf.keras.backend.clear_session()\n        \n        return fitness\n    \n    def select_parent(self):\n        \"\"\"Select parent using tournament selection.\"\"\"\n        tournament = random.sample(list(enumerate(self.population)), Config.tournament_size)\n        fitness_scores = []\n        \n        for idx, genotype in tournament:\n            # Try to retrieve cached fitness if available\n            if idx < len(self.fitness_history) and self.fitness_history[idx] is not None:\n                fitness = self.fitness_history[idx]\n            else:\n                fitness = self.evaluate_fitness(genotype)\n                # Cache the fitness\n                if idx >= len(self.fitness_history):\n                    self.fitness_history.extend([None] * (idx + 1 - len(self.fitness_history)))\n                self.fitness_history[idx] = fitness\n            \n            fitness_scores.append((genotype, fitness))\n        \n        # Return the genotype with the best fitness\n        return max(fitness_scores, key=lambda x: x[1])[0]\n    \n    def evolve(self):\n        \"\"\"Run the evolutionary search for optimal architectures.\"\"\"\n        print(\"Initializing population...\")\n        self.initialize_population()\n        \n        for generation in range(Config.generations):\n            print(f\"\\nGeneration {generation+1}/{Config.generations}\")\n            \n            # Evaluate fitness for all individuals in the population\n            fitness_scores = []\n            for i, genotype in enumerate(tqdm(self.population, desc=\"Evaluating fitness\")):\n                fitness = self.evaluate_fitness(genotype, epochs=Config.nas_epochs)\n                fitness_scores.append(fitness)\n                \n                # Update best genotype if necessary\n                if fitness > self.best_fitness:\n                    self.best_fitness = fitness\n                    self.best_genotype = deepcopy(genotype)\n            \n            self.fitness_history = fitness_scores\n            \n            # Create next generation through selection, crossover, and mutation\n            new_population = []\n            \n            # Elitism: keep the best individual\n            best_idx = np.argmax(fitness_scores)\n            new_population.append(deepcopy(self.population[best_idx]))\n            \n            # Generate rest of the population\n            while len(new_population) < Config.population_size:\n                # Select parents\n                parent1 = self.select_parent()\n                parent2 = self.select_parent()\n                \n                # Crossover\n                if random.random() < 0.5:\n                    child = parent1.crossover(parent2)\n                else:\n                    child = deepcopy(random.choice([parent1, parent2]))\n                \n                # Mutation\n                if random.random() < Config.mutation_rate:\n                    child = child.mutate()\n                \n                new_population.append(child)\n            \n            # Replace old population\n            self.population = new_population\n            \n            # Print statistics\n            avg_fitness = np.mean(fitness_scores)\n            best_fitness = np.max(fitness_scores)\n            print(f\"Generation {generation+1} stats: Avg fitness = {avg_fitness:.4f}, Best fitness = {best_fitness:.4f}\")\n            print(f\"Best genotype so far: {self.best_genotype}\")\n        \n        print(\"\\nEvolution complete. Best genotype:\")\n        print(self.best_genotype)\n        return self.best_genotype\n    \n    def train_best_model(self, epochs=10, batch_size=32, save_interval=5):\n        \"\"\"Train the best discovered architecture for more epochs.\"\"\"\n        if self.best_genotype is None:\n            raise ValueError(\"No best genotype found. Run evolve() first.\")\n        \n        # Build generator with best genotype\n        nas_gen = NASGANGenerator(self.best_genotype)\n        generator = nas_gen.build_model()\n        \n        # Build discriminator\n        discriminator = self._build_discriminator()\n        discriminator.compile(\n            loss='binary_crossentropy',\n            optimizer=Adam(0.0002, 0.5),\n            metrics=['accuracy']\n        )\n        \n        # Create GAN\n        discriminator.trainable = False\n        z = Input(shape=(self.latent_dim,))\n        img = generator(z)\n        valid = discriminator(img)\n        combined = Model(z, valid)\n        combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n        \n        # Training history\n        d_losses = []\n        g_losses = []\n        d_accs = []\n        \n        half_batch = batch_size // 2\n        \n        for epoch in range(epochs):\n            # ---------------------\n            #  Train Discriminator\n            # ---------------------\n            \n            # Select a random batch of images\n            idx = np.random.randint(0, self.X_train.shape[0], half_batch)\n            imgs = self.X_train[idx]\n            \n            # Generate a batch of new images\n            noise = np.random.normal(0, 1, (half_batch, self.latent_dim))\n            gen_imgs = generator.predict(noise)\n            \n            # Train the discriminator\n            d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n            d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            \n            # ---------------------\n            #  Train Generator\n            # ---------------------\n            \n            # Train the generator to fool the discriminator\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            g_loss = combined.train_on_batch(noise, np.ones((batch_size, 1)))\n            \n            # Save losses and accuracy for plotting\n            d_losses.append(d_loss[0])\n            d_accs.append(d_loss[1])\n            g_losses.append(g_loss)\n            \n            # Print progress\n            # Extract loss values\n            d_loss_value = float(d_loss[0]) if isinstance(d_loss, (list, tuple, np.ndarray)) else float(d_loss)\n            d_acc_value = float(d_loss[1]) if isinstance(d_loss, (list, tuple, np.ndarray)) and len(d_loss) > 1 else 0.0\n            g_loss_value = float(g_loss[0]) if isinstance(g_loss, (list, tuple, np.ndarray)) else float(g_loss)\n\n            \n            # Print progress\n            print(f\"{epoch}/{epochs} [D loss: {d_loss_value:.4f}, acc.: {100*d_acc_value:.2f}%] [G loss: {g_loss_value:.4f}]\")\n\n            # If at save interval => save generated image samples\n            if epoch % save_interval == 0:\n                self.save_imgs(generator, epoch, \"nas\")\n        \n        # Plot training history\n        self.plot_history(d_losses, g_losses, d_accs, \"nas\")\n        \n        return generator, discriminator, (d_losses, g_losses, d_accs)\n    \n    def save_imgs(self, generator, epoch, prefix, examples=10):\n        \"\"\"Save generated images.\"\"\"\n        noise = np.random.normal(0, 1, (examples, self.latent_dim))\n        generated_images = generator.predict(noise)\n        \n        # Rescale images from [-1, 1] to [0, 1]\n        generated_images = (generated_images + 1) / 2.0\n        \n        plt.figure(figsize=(10, 4))\n        for i in range(examples):\n            plt.subplot(2, 5, i+1)\n            plt.imshow(generated_images[i])\n            plt.axis('off')\n        plt.tight_layout()\n        plt.savefig(f'{prefix}_gan_generated_image_epoch_{epoch}.png')\n        plt.close()\n        \n        return generated_images\n    \n    def plot_history(self, d_losses, g_losses, d_accs, prefix):\n        plt.figure(figsize=(15, 5))\n        \n        # Plot discriminator loss\n        plt.subplot(1, 3, 1)\n        plt.plot(d_losses)\n        plt.title('Discriminator Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        \n        # Plot generator loss\n        plt.subplot(1, 3, 2)\n        plt.plot(g_losses)\n        plt.title('Generator Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        \n        # Plot discriminator accuracy\n        plt.subplot(1, 3, 3)\n        plt.plot(d_accs)\n        plt.title('Discriminator Accuracy')\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy')\n        \n        plt.tight_layout()\n        plt.savefig(f'{prefix}_gan_training_history.png')\n        plt.close()\n    \n    def generate_augmented_images(self, num_images=5000):\n        \"\"\"Generate augmented images using the best architecture.\"\"\"\n        if self.best_genotype is None:\n            raise ValueError(\"No best genotype found. Run evolve() first.\")\n        \n        nas_gen = NASGANGenerator(self.best_genotype)\n        generator = nas_gen.build_model()\n        \n        noise = np.random.normal(0, 1, (num_images, self.latent_dim))\n        generated_images = generator.predict(noise)\n        \n        # Convert from [-1, 1] to [0, 1]\n        generated_images = (generated_images + 1) / 2.0\n        \n        return generated_images\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T02:34:00.980249Z","iopub.execute_input":"2025-04-25T02:34:00.980460Z","iopub.status.idle":"2025-04-25T02:34:01.010333Z","shell.execute_reply.started":"2025-04-25T02:34:00.980435Z","shell.execute_reply":"2025-04-25T02:34:01.009614Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def build_classifier(input_shape=(64, 64, 3), augmented=False):\n    \"\"\"Build a CNN classifier for malaria detection.\"\"\"\n    model = Sequential()\n    \n    # First convolutional block\n    model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    \n    # Second convolutional block\n    model.add(Conv2D(64, (3, 3), padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    \n    # Third convolutional block\n    model.add(Conv2D(128, (3, 3), padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    \n    # Classification block\n    model.add(Flatten())\n    model.add(Dense(256))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    # Compile model\n    model.compile(\n        optimizer=Adam(learning_rate=0.0001),  # Changed from lr to learning_rate\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T02:34:01.011101Z","iopub.execute_input":"2025-04-25T02:34:01.011415Z","iopub.status.idle":"2025-04-25T02:34:01.023876Z","shell.execute_reply.started":"2025-04-25T02:34:01.011390Z","shell.execute_reply":"2025-04-25T02:34:01.023277Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Evaluation function\ndef evaluate_classifier(model, X_test, y_test):\n    \"\"\"Evaluate classifier performance.\"\"\"\n    # Predict on test set\n    y_pred_prob = model.predict(X_test)\n    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n    \n    # Calculate metrics\n    print(\"Classification Report:\")\n    print(classification_report(y_test, y_pred))\n    \n    # Confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    tick_marks = np.arange(2)\n    plt.xticks(tick_marks, ['Uninfected', 'Parasite'])\n    plt.yticks(tick_marks, ['Uninfected', 'Parasite'])\n    \n    # Add text annotations\n    thresh = cm.max() / 2\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(j, i, cm[i, j],\n                    horizontalalignment=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    \n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    plt.savefig('confusion_matrix.png')\n    plt.close()\n    \n    return y_pred, cm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T02:34:01.025673Z","iopub.execute_input":"2025-04-25T02:34:01.025866Z","iopub.status.idle":"2025-04-25T02:34:01.040502Z","shell.execute_reply.started":"2025-04-25T02:34:01.025852Z","shell.execute_reply":"2025-04-25T02:34:01.039889Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Main execution function\ndef run_experiment():\n    \"\"\"Run the complete experiment.\"\"\"\n    # Load data\n    print(\"Loading data...\")\n    train_gen, test_gen = create_data_generators()\n    X_train, y_train = load_data_from_generator(train_gen)\n    X_test, y_test = load_data_from_generator(test_gen)\n    \n    print(f\"Training data shape: {X_train.shape}\")\n    print(f\"Test data shape: {X_test.shape}\")\n    \n    # 1. Train a standard CNN classifier (baseline)\n    print(\"\\n1. Training baseline classifier...\")\n    baseline_classifier = build_classifier()\n    baseline_history = baseline_classifier.fit(\n        X_train, y_train,\n        epochs=15,\n        batch_size=Config.batch_size,\n        validation_data=(X_test, y_test),\n        verbose=1\n    )\n    \n    print(\"\\nEvaluating baseline classifier...\")\n    baseline_preds, baseline_cm = evaluate_classifier(baseline_classifier, X_test, y_test)\n    \n    # 2. Train a standard GAN and use it for data augmentation\n    print(\"\\n2. Training standard GAN...\")\n    standard_gan = StandardGAN(X_train)\n    standard_gan.train(epochs=Config.standard_gan_epochs, batch_size=Config.batch_size)\n    \n    # Generate augmented data\n    print(\"Generating augmented images from standard GAN...\")\n    standard_gan_images = standard_gan.generate_augmented_images(num_images=len(X_train))\n    \n    # Create augmented dataset\n    standard_augmented_X = np.concatenate([X_train, standard_gan_images*2-1], axis=0)  # Scale back to [-1, 1]\n    standard_augmented_y = np.concatenate([y_train, np.zeros(len(standard_gan_images))], axis=0)  # Label generated images as uninfected\n    \n    # Train classifier with standard GAN augmentation\n    print(\"Training classifier with standard GAN augmentation...\")\n    standard_gan_classifier = build_classifier()\n    standard_gan_history = standard_gan_classifier.fit(\n        standard_augmented_X, standard_augmented_y,\n        epochs=15,\n        batch_size=Config.batch_size,\n        validation_data=(X_test, y_test),\n        verbose=1\n    )\n    \n    print(\"\\nEvaluating standard GAN-augmented classifier...\")\n    standard_gan_preds, standard_gan_cm = evaluate_classifier(standard_gan_classifier, X_test, y_test)\n    \n    # 3. Run NAS to find optimal GAN architecture\n    print(\"\\n3. Running Neural Architecture Search for GAN...\")\n    nas_gan = NASGAN(X_train)\n    best_genotype = nas_gan.evolve()\n    \n    # Train the best architecture for more epochs\n    print(\"Training best NAS-discovered GAN architecture...\")\n    best_generator, _, _ = nas_gan.train_best_model(epochs=Config.final_epochs)\n    \n    # Generate augmented data with the NAS-GAN\n    print(\"Generating augmented images from NAS-GAN...\")\n    nas_gan_images = nas_gan.generate_augmented_images(num_images=len(X_train))\n    \n    # Create augmented dataset\n    nas_augmented_X = np.concatenate([X_train, nas_gan_images*2-1], axis=0)  # Scale back to [-1, 1]\n    nas_augmented_y = np.concatenate([y_train, np.zeros(len(nas_gan_images))], axis=0)  # Label generated images as uninfected\n    \n    # Train classifier with NAS-GAN augmentation\n    print(\"Training classifier with NAS-GAN augmentation...\")\n    nas_gan_classifier = build_classifier()\n    nas_gan_history = nas_gan_classifier.fit(\n        nas_augmented_X, nas_augmented_y,\n        epochs=15,\n        batch_size=Config.batch_size,\n        validation_data=(X_test, y_test),\n        verbose=1\n    )\n    \n    print(\"\\nEvaluating NAS-GAN-augmented classifier...\")\n    nas_gan_preds, nas_gan_cm = evaluate_classifier(nas_gan_classifier, X_test, y_test)\n    \n    # 4. Compare results\n    print(\"\\n4. Comparing results of all models...\")\n    \n    # Plot training histories\n    plt.figure(figsize=(12, 5))\n    \n    # Accuracy comparison\n    plt.subplot(1, 2, 1)\n    plt.plot(baseline_history.history['val_accuracy'], label='Baseline')\n    plt.plot(standard_gan_history.history['val_accuracy'], label='Standard GAN')\n    plt.plot(nas_gan_history.history['val_accuracy'], label='NAS-GAN')\n    plt.title('Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    # Loss comparison\n    plt.subplot(1, 2, 2)\n    plt.plot(baseline_history.history['val_loss'], label='Baseline')\n    plt.plot(standard_gan_history.history['val_loss'], label='Standard GAN')\n    plt.plot(nas_gan_history.history['val_loss'], label='NAS-GAN')\n    plt.title('Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig('model_comparison.png')\n    plt.close()\n    \n    print(\"\\nExperiment complete! Results saved to disk.\")\n    \n    return {\n        'baseline_classifier': baseline_classifier,\n        'standard_gan_classifier': standard_gan_classifier,\n        'nas_gan_classifier': nas_gan_classifier,\n        'best_genotype': best_genotype\n    }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T02:34:01.041137Z","iopub.execute_input":"2025-04-25T02:34:01.041353Z","iopub.status.idle":"2025-04-25T02:34:01.059610Z","shell.execute_reply.started":"2025-04-25T02:34:01.041339Z","shell.execute_reply":"2025-04-25T02:34:01.059022Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# If this script is run directly, execute the experiment\nif __name__ == \"__main__\":\n    print(\"Starting NAS-GAN experiment for malaria detection...\")\n    start_time = time.time()\n    results = run_experiment()\n    end_time = time.time()\n    \n    print(f\"\\nTotal experiment time: {(end_time - start_time)/60:.2f} minutes\")\n    print(\"Best GAN architecture discovered:\")\n    print(results['best_genotype'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T02:34:01.060266Z","iopub.execute_input":"2025-04-25T02:34:01.060453Z","iopub.status.idle":"2025-04-25T02:59:02.430406Z","shell.execute_reply.started":"2025-04-25T02:34:01.060438Z","shell.execute_reply":"2025-04-25T02:59:02.429740Z"}},"outputs":[{"name":"stdout","text":"Starting NAS-GAN experiment for malaria detection...\nLoading data...\nFound 416 images belonging to 2 classes.\nFound 134 images belonging to 2 classes.\nTraining data shape: (416, 64, 64, 3)\nTest data shape: (134, 64, 64, 3)\n\n1. Training baseline classifier...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\nI0000 00:00:1745548446.508946      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1745548446.509619      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1745548452.121529     109 service.cc:148] XLA service 0x7bbb740091a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1745548452.122352     109 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1745548452.122374     109 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1745548452.597786     109 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m 7/13\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5094 - loss: 0.8792  ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1745548458.151022     109 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m13/13\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5407 - loss: 0.8131 - val_accuracy: 0.3284 - val_loss: 0.7364\nEpoch 2/15\n\u001b[1m13/13\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6766 - loss: 0.6045 - val_accuracy: 0.3209 - val_loss: 0.8198\nEpoch 3/15\n\u001b[1m13/13\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7872 - loss: 0.4696 - val_accuracy: 0.3209 - val_loss: 0.9119\nEpoch 4/15\n\u001b[1m13/13\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7886 - loss: 0.4379 - val_accuracy: 0.3209 - val_loss: 1.0286\nEpoch 5/15\n\u001b[1m13/13\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7601 - loss: 0.4611 - val_accuracy: 0.3209 - val_loss: 1.1672\nEpoch 6/15\n\u001b[1m13/13\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8365 - loss: 0.3512 - val_accuracy: 0.3209 - val_loss: 1.3237\nEpoch 7/15\n\u001b[1m13/13\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8461 - loss: 0.3412 - val_accuracy: 0.3209 - val_loss: 1.4405\nEpoch 8/15\n\u001b[1m13/13\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8553 - loss: 0.3380 - val_accuracy: 0.3209 - val_loss: 1.5768\nEpoch 9/15\n\u001b[1m13/13\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8553 - loss: 0.2910 - val_accuracy: 0.3209 - val_loss: 1.7349\nEpoch 10/15\n\u001b[1m13/13\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8811 - loss: 0.2611 - val_accuracy: 0.3209 - val_loss: 1.9128\nEpoch 11/15\n\u001b[1m13/13\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8531 - loss: 0.3115 - val_accuracy: 0.3209 - val_loss: 2.0567\nEpoch 12/15\n\u001b[1m13/13\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9045 - loss: 0.2496 - val_accuracy: 0.3209 - val_loss: 2.1880\nEpoch 13/15\n\u001b[1m13/13\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8651 - loss: 0.3000 - val_accuracy: 0.3209 - val_loss: 2.2720\nEpoch 14/15\n\u001b[1m13/13\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8450 - loss: 0.2858 - val_accuracy: 0.3209 - val_loss: 2.3884\nEpoch 15/15\n\u001b[1m13/13\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8855 - loss: 0.2610 - val_accuracy: 0.3209 - val_loss: 2.5868\n\nEvaluating baseline classifier...\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step\nClassification Report:\n              precision    recall  f1-score   support\n\n         0.0       0.00      0.00      0.00        91\n         1.0       0.32      1.00      0.49        43\n\n    accuracy                           0.32       134\n   macro avg       0.16      0.50      0.24       134\nweighted avg       0.10      0.32      0.16       134\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\n2. Training standard GAN...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py:75: UserWarning: The model does not have any trainable weights.\n  warnings.warn(\"The model does not have any trainable weights.\")\n","output_type":"stream"},{"name":"stdout","text":"0/10 [D loss: 0.7025, acc.: 37.50%] [G loss: 0.6983]\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n1/10 [D loss: 0.7023, acc.: 40.62%] [G loss: 0.7008]\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n2/10 [D loss: 0.7001, acc.: 44.58%] [G loss: 0.6992]\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n3/10 [D loss: 0.6996, acc.: 45.70%] [G loss: 0.6990]\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n4/10 [D loss: 0.6995, acc.: 47.92%] [G loss: 0.6990]\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n5/10 [D loss: 0.6983, acc.: 50.78%] [G loss: 0.6979]\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n6/10 [D loss: 0.6983, acc.: 49.98%] [G loss: 0.6981]\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n7/10 [D loss: 0.6983, acc.: 49.18%] [G loss: 0.6980]\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n8/10 [D loss: 0.6984, acc.: 48.39%] [G loss: 0.6981]\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n9/10 [D loss: 0.6987, acc.: 47.75%] [G loss: 0.6985]\nGenerating augmented images from standard GAN...\n\u001b[1m13/13\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\nTraining classifier with standard GAN augmentation...\nEpoch 1/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 47ms/step - accuracy: 0.6874 - loss: 0.6950 - val_accuracy: 0.6791 - val_loss: 0.6226\nEpoch 2/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8230 - loss: 0.4415 - val_accuracy: 0.6791 - val_loss: 0.6505\nEpoch 3/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8233 - loss: 0.4176 - val_accuracy: 0.6791 - val_loss: 0.7629\nEpoch 4/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8709 - loss: 0.3425 - val_accuracy: 0.6791 - val_loss: 0.8793\nEpoch 5/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8693 - loss: 0.3287 - val_accuracy: 0.6791 - val_loss: 0.9595\nEpoch 6/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8952 - loss: 0.2849 - val_accuracy: 0.6791 - val_loss: 0.9857\nEpoch 7/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9020 - loss: 0.2385 - val_accuracy: 0.6791 - val_loss: 0.9283\nEpoch 8/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9022 - loss: 0.2318 - val_accuracy: 0.6791 - val_loss: 0.9156\nEpoch 9/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9068 - loss: 0.2320 - val_accuracy: 0.6791 - val_loss: 0.7890\nEpoch 10/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9246 - loss: 0.2017 - val_accuracy: 0.6791 - val_loss: 0.6823\nEpoch 11/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9244 - loss: 0.1917 - val_accuracy: 0.7015 - val_loss: 0.6152\nEpoch 12/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9246 - loss: 0.2068 - val_accuracy: 0.6866 - val_loss: 0.5987\nEpoch 13/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9273 - loss: 0.2057 - val_accuracy: 0.6045 - val_loss: 0.6585\nEpoch 14/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9375 - loss: 0.1628 - val_accuracy: 0.5746 - val_loss: 0.7398\nEpoch 15/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9106 - loss: 0.2041 - val_accuracy: 0.5299 - val_loss: 0.8787\n\nEvaluating standard GAN-augmented classifier...\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step\nClassification Report:\n              precision    recall  f1-score   support\n\n         0.0       0.89      0.35      0.50        91\n         1.0       0.40      0.91      0.55        43\n\n    accuracy                           0.53       134\n   macro avg       0.64      0.63      0.53       134\nweighted avg       0.73      0.53      0.52       134\n\n\n3. Running Neural Architecture Search for GAN...\nInitializing population...\n\nGeneration 1/5\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py:75: UserWarning: The model does not have any trainable weights.\n  warnings.warn(\"The model does not have any trainable weights.\")\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  10%|         | 1/10 [00:21<03:13, 21.47s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 877ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  20%|        | 2/10 [00:39<02:34, 19.35s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  30%|       | 3/10 [01:10<02:52, 24.62s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  40%|      | 4/10 [01:56<03:19, 33.25s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  50%|     | 5/10 [02:27<02:41, 32.34s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  60%|    | 6/10 [02:57<02:06, 31.54s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  70%|   | 7/10 [03:28<01:34, 31.49s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  80%|  | 8/10 [03:53<00:58, 29.36s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  90%| | 9/10 [04:14<00:26, 26.70s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness: 100%|| 10/10 [04:35<00:00, 27.52s/it]\n","output_type":"stream"},{"name":"stdout","text":"Generation 1 stats: Avg fitness = -0.4916, Best fitness = -0.4338\nBest genotype so far: Node 2: (0->2, dil_conv5x5) (1->2, skip_connect) \nNode 3: (1->3, conv7x7) \nNode 4: (2->4, dil_conv5x5) \nNode 5: (2->5, conv7x7) (1->5, skip_connect) \n\nGeneration 2/5\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:   0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  10%|         | 1/10 [00:26<03:58, 26.56s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  20%|        | 2/10 [00:53<03:33, 26.72s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  30%|       | 3/10 [01:20<03:08, 26.97s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  40%|      | 4/10 [01:47<02:42, 27.09s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  50%|     | 5/10 [02:16<02:18, 27.70s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  60%|    | 6/10 [02:55<02:06, 31.57s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  70%|   | 7/10 [03:17<01:24, 28.32s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  80%|  | 8/10 [03:43<00:55, 27.72s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  90%| | 9/10 [04:10<00:27, 27.25s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness: 100%|| 10/10 [04:45<00:00, 28.58s/it]\n","output_type":"stream"},{"name":"stdout","text":"Generation 2 stats: Avg fitness = -0.5328, Best fitness = -0.4982\nBest genotype so far: Node 2: (0->2, dil_conv5x5) (1->2, skip_connect) \nNode 3: (1->3, conv7x7) \nNode 4: (2->4, dil_conv5x5) \nNode 5: (2->5, conv7x7) (1->5, skip_connect) \n\nGeneration 3/5\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:   0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  10%|         | 1/10 [00:26<03:56, 26.30s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  20%|        | 2/10 [00:52<03:31, 26.38s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  30%|       | 3/10 [01:19<03:05, 26.43s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  40%|      | 4/10 [01:41<02:28, 24.76s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  50%|     | 5/10 [02:07<02:06, 25.31s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  60%|    | 6/10 [02:43<01:55, 28.96s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  70%|   | 7/10 [03:11<01:25, 28.55s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  80%|  | 8/10 [03:38<00:56, 28.03s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  90%| | 9/10 [04:05<00:27, 27.69s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness: 100%|| 10/10 [04:32<00:00, 27.20s/it]\n","output_type":"stream"},{"name":"stdout","text":"Generation 3 stats: Avg fitness = -0.5124, Best fitness = -0.4677\nBest genotype so far: Node 2: (0->2, dil_conv5x5) (1->2, skip_connect) \nNode 3: (1->3, conv7x7) \nNode 4: (2->4, dil_conv5x5) \nNode 5: (2->5, conv7x7) (1->5, skip_connect) \n\nGeneration 4/5\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:   0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  10%|         | 1/10 [00:26<04:02, 26.94s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  20%|        | 2/10 [00:53<03:35, 26.91s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  30%|       | 3/10 [01:20<03:08, 26.88s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  40%|      | 4/10 [01:43<02:31, 25.33s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  50%|     | 5/10 [02:10<02:09, 25.96s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  60%|    | 6/10 [02:38<01:45, 26.42s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  70%|   | 7/10 [03:07<01:22, 27.47s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  80%|  | 8/10 [03:37<00:56, 28.35s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  90%| | 9/10 [04:05<00:28, 28.04s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness: 100%|| 10/10 [04:32<00:00, 27.28s/it]\n","output_type":"stream"},{"name":"stdout","text":"Generation 4 stats: Avg fitness = -0.5299, Best fitness = -0.4734\nBest genotype so far: Node 2: (0->2, dil_conv5x5) (1->2, skip_connect) \nNode 3: (1->3, conv7x7) \nNode 4: (2->4, dil_conv5x5) \nNode 5: (2->5, conv7x7) (1->5, skip_connect) \n\nGeneration 5/5\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:   0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  10%|         | 1/10 [00:28<04:20, 28.99s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  20%|        | 2/10 [00:56<03:44, 28.11s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  30%|       | 3/10 [01:24<03:15, 27.91s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  40%|      | 4/10 [01:52<02:49, 28.21s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  50%|     | 5/10 [02:20<02:19, 28.00s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  60%|    | 6/10 [02:49<01:53, 28.41s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  70%|   | 7/10 [03:18<01:25, 28.66s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  80%|  | 8/10 [03:46<00:56, 28.35s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness:  90%| | 9/10 [04:14<00:28, 28.23s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"Evaluating fitness: 100%|| 10/10 [04:42<00:00, 28.24s/it]\n","output_type":"stream"},{"name":"stdout","text":"Generation 5 stats: Avg fitness = -0.5151, Best fitness = -0.4293\nBest genotype so far: Node 2: (0->2, dil_conv5x5) (1->2, skip_connect) \nNode 3: (2->3, conv3x3) \nNode 4: (0->4, conv3x3) \nNode 5: (2->5, conv7x7) (1->5, skip_connect) \n\nEvolution complete. Best genotype:\nNode 2: (0->2, dil_conv5x5) (1->2, skip_connect) \nNode 3: (2->3, conv3x3) \nNode 4: (0->4, conv3x3) \nNode 5: (2->5, conv7x7) (1->5, skip_connect) \nTraining best NAS-discovered GAN architecture...\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n0/10 [D loss: 0.6746, acc.: 60.94%] [G loss: 0.6849]\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n1/10 [D loss: 0.6844, acc.: 49.22%] [G loss: 0.6890]\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n2/10 [D loss: 0.6894, acc.: 45.83%] [G loss: 0.6933]\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n3/10 [D loss: 0.6935, acc.: 45.20%] [G loss: 0.6978]\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n4/10 [D loss: 0.6989, acc.: 44.20%] [G loss: 0.7030]\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n5/10 [D loss: 0.7039, acc.: 44.11%] [G loss: 0.7075]\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n6/10 [D loss: 0.7077, acc.: 44.04%] [G loss: 0.7110]\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n7/10 [D loss: 0.7104, acc.: 44.40%] [G loss: 0.7135]\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n8/10 [D loss: 0.7137, acc.: 44.32%] [G loss: 0.7163]\n\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n9/10 [D loss: 0.7160, acc.: 44.26%] [G loss: 0.7181]\nGenerating augmented images from NAS-GAN...\n\u001b[1m13/13\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\nTraining classifier with NAS-GAN augmentation...\nEpoch 1/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 47ms/step - accuracy: 0.7117 - loss: 0.6222 - val_accuracy: 0.6791 - val_loss: 0.6272\nEpoch 2/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8143 - loss: 0.4159 - val_accuracy: 0.6791 - val_loss: 0.6554\nEpoch 3/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8487 - loss: 0.3365 - val_accuracy: 0.6791 - val_loss: 0.7726\nEpoch 4/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8522 - loss: 0.2946 - val_accuracy: 0.6791 - val_loss: 0.9261\nEpoch 5/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8552 - loss: 0.2992 - val_accuracy: 0.6791 - val_loss: 1.0858\nEpoch 6/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8691 - loss: 0.2750 - val_accuracy: 0.6791 - val_loss: 1.1779\nEpoch 7/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8864 - loss: 0.2538 - val_accuracy: 0.6791 - val_loss: 1.2423\nEpoch 8/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8752 - loss: 0.2450 - val_accuracy: 0.6791 - val_loss: 1.2740\nEpoch 9/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8990 - loss: 0.2365 - val_accuracy: 0.6791 - val_loss: 1.2862\nEpoch 10/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9098 - loss: 0.2260 - val_accuracy: 0.6791 - val_loss: 1.3188\nEpoch 11/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9067 - loss: 0.2157 - val_accuracy: 0.6791 - val_loss: 1.2827\nEpoch 12/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8979 - loss: 0.2232 - val_accuracy: 0.6791 - val_loss: 1.1499\nEpoch 13/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9011 - loss: 0.2113 - val_accuracy: 0.6791 - val_loss: 1.0079\nEpoch 14/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9079 - loss: 0.2124 - val_accuracy: 0.6493 - val_loss: 0.8620\nEpoch 15/15\n\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9240 - loss: 0.1816 - val_accuracy: 0.5746 - val_loss: 0.8403\n\nEvaluating NAS-GAN-augmented classifier...\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step\nClassification Report:\n              precision    recall  f1-score   support\n\n         0.0       0.65      0.81      0.72        91\n         1.0       0.15      0.07      0.10        43\n\n    accuracy                           0.57       134\n   macro avg       0.40      0.44      0.41       134\nweighted avg       0.49      0.57      0.52       134\n\n\n4. Comparing results of all models...\n\nExperiment complete! Results saved to disk.\n\nTotal experiment time: 25.02 minutes\nBest GAN architecture discovered:\nNode 2: (0->2, dil_conv5x5) (1->2, skip_connect) \nNode 3: (2->3, conv3x3) \nNode 4: (0->4, conv3x3) \nNode 5: (2->5, conv7x7) (1->5, skip_connect) \n","output_type":"stream"}],"execution_count":12}]}